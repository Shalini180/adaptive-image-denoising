"""
Complete Adaptive Image Denoising System
Integration of all optimized components for production deployment

System Components:
1. Noise Type Detection
2. Empirically Optimized Method Weights
3. Uncertainty Quantification
4. Adaptive Refinement Strategy
5. Performance Monitoring

Usage:
    denoiser = AdaptiveImageDenoiser()
    result = denoiser.denoise_image(noisy_image)
"""

import numpy as np
import cv2
import json
from pathlib import Path
import time
from datetime import datetime
from scipy import stats, ndimage
from sklearn.ensemble import RandomForestClassifier
import pickle
import warnings
warnings.filterwarnings('ignore')

# Import core components
from core_methods import CoreDenoisingMethods

class NoiseDetector:
    """
    Advanced noise type detection using statistical analysis
    Implements the detection framework designed in Phase 1.1
    """
    
    def __init__(self):
        self.noise_features_config = {
            'gaussian': {
                'features': ['spatial_uniformity', 'normality_test', 'variance_consistency'],
                'thresholds': [0.7, 0.05, 0.8]
            },
            'salt_pepper': {
                'features': ['impulse_ratio', 'binary_distribution', 'spatial_clustering'],
                'thresholds': [0.01, 0.8, 0.3]
            },
            'speckle': {
                'features': ['multiplicative_test', 'gamma_fit', 'texture_correlation'],
                'thresholds': [0.6, 0.05, 0.7]
            },
            'uniform': {
                'features': ['range_consistency', 'uniformity_test', 'frequency_distribution'],
                'thresholds': [0.8, 0.05, 0.7]
            },
            'poisson': {
                'features': ['variance_mean_ratio', 'poisson_fit', 'intensity_dependence'],
                'thresholds': [0.8, 0.05, 0.6]
            }
        }
    
    def extract_noise_features(self, image):
        """Extract comprehensive noise features for classification"""
        
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image.copy()
        
        features = {}
        
        # 1. Spatial uniformity (for Gaussian)
        patches = self._extract_patches(gray, size=16)
        patch_variances = [np.var(cv2.Laplacian(patch, cv2.CV_64F)) for patch in patches]
        if len(patch_variances) > 1:
            features['spatial_uniformity'] = 1.0 - (np.std(patch_variances) / (np.mean(patch_variances) + 1e-8))
        else:
            features['spatial_uniformity'] = 0.5
        
        # 2. Normality test
        residuals = gray - cv2.GaussianBlur(gray, (5, 5), 1.0)
        _, p_value = stats.normaltest(residuals.flatten())
        features['normality_test'] = 1.0 - min(p_value, 1.0)
        
        # 3. Impulse noise detection
        diff = np.abs(cv2.medianBlur(gray, 5).astype(np.float32) - gray.astype(np.float32))
        impulse_pixels = np.sum(diff > 50)
        features['impulse_ratio'] = impulse_pixels / gray.size
        
        # 4. Variance consistency (Gaussian characteristic)
        bright_mask = gray > np.mean(gray)
        dark_mask = gray <= np.mean(gray)
        if np.sum(bright_mask) > 0 and np.sum(dark_mask) > 0:
            bright_var = np.var(residuals[bright_mask])
            dark_var = np.var(residuals[dark_mask])
            features['variance_consistency'] = 1.0 - abs(bright_var - dark_var) / (bright_var + dark_var + 1e-8)
        else:
            features['variance_consistency'] = 0.5
        
        # 5. Multiplicative test (for speckle)
        smooth = cv2.GaussianBlur(gray, (9, 9), 2.0)
        ratio = gray.astype(np.float32) / (smooth.astype(np.float32) + 1e-8)
        features['multiplicative_test'] = np.std(ratio) / (np.mean(ratio) + 1e-8)
        
        # 6. Uniformity test
        hist, _ = np.histogram(residuals.flatten(), bins=50)
        hist_norm = hist / np.sum(hist)
        uniform_hist = np.ones_like(hist_norm) / len(hist_norm)
        features['uniformity_test'] = 1.0 - np.sum(np.abs(hist_norm - uniform_hist)) / 2.0
        
        # 7. Poisson characteristics
        features['variance_mean_ratio'] = np.var(gray) / (np.mean(gray) + 1e-8)
        
        return features
    
    def _extract_patches(self, image, size=16):
        """Extract non-overlapping patches from image"""
        patches = []
        h, w = image.shape
        
        for i in range(0, h - size, size):
            for j in range(0, w - size, size):
                patch = image[i:i+size, j:j+size]
                if patch.shape == (size, size):
                    patches.append(patch)
        
        return patches[:20]  # Limit number of patches
    
    def detect_noise_type(self, image):
        """Detect primary noise type using rule-based classification"""
        
        features = self.extract_noise_features(image)
        
        # Rule-based classification
        noise_scores = {}
        
        for noise_type, config in self.noise_features_config.items():
            score = 0.0
            feature_count = 0
            
            for feature_name, threshold in zip(config['features'], config['thresholds']):
                if feature_name in features:
                    feature_value = features[feature_name]
                    
                    # Score based on how well feature matches expected pattern
                    if noise_type == 'gaussian':
                        if feature_name == 'spatial_uniformity':
                            score += 1.0 if feature_value > threshold else feature_value / threshold
                        elif feature_name == 'normality_test':
                            score += 1.0 if feature_value < threshold else (1.0 - feature_value)
                        elif feature_name == 'variance_consistency':
                            score += 1.0 if feature_value > threshold else feature_value / threshold
                    
                    elif noise_type == 'salt_pepper':
                        if feature_name == 'impulse_ratio':
                            score += min(feature_value / threshold, 1.0) if feature_value > threshold else 0.0
                    
                    elif noise_type == 'speckle':
                        if feature_name == 'multiplicative_test':
                            score += min(feature_value / threshold, 1.0) if feature_value > threshold else 0.0
                    
                    elif noise_type == 'uniform':
                        if feature_name == 'uniformity_test':
                            score += 1.0 if feature_value < threshold else (1.0 - feature_value)
                    
                    elif noise_type == 'poisson':
                        if feature_name == 'variance_mean_ratio':
                            # For Poisson, variance should approximately equal mean
                            ideal_ratio = 1.0
                            score += 1.0 - abs(feature_value - ideal_ratio) / (ideal_ratio + 1.0)
                    
                    feature_count += 1
            
            noise_scores[noise_type] = score / max(feature_count, 1)
        
        # Find best match
        primary_noise = max(noise_scores, key=noise_scores.get)
        confidence = noise_scores[primary_noise]
        
        return {
            'primary_noise_type': primary_noise,
            'confidence': confidence,
            'all_scores': noise_scores,
            'features': features
        }
    
    def estimate_noise_level(self, image):
        """Estimate noise level using robust MAD estimator"""
        
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image.copy()
        
        # Use Laplacian for edge detection
        laplacian = cv2.Laplacian(gray, cv2.CV_64F)
        
        # Robust noise estimation using Median Absolute Deviation (MAD)
        sigma = np.median(np.abs(laplacian - np.median(laplacian))) / 0.6745
        
        # Normalize to [0, 1] range
        normalized_sigma = min(sigma / 100.0, 1.0)  # Assume max noise level corresponds to sigma=100
        
        return normalized_sigma

class AdaptiveImageDenoiser:
    """
    Complete adaptive image denoising system with empirically optimized parameters
    Integrates all research phases into production-ready system
    """
    
    def __init__(self, config_dir="experiments"):
        self.config_dir = Path(config_dir)
        
        # Initialize components
        self.core_methods = CoreDenoisingMethods()
        self.noise_detector = NoiseDetector()
        
        # Load empirically optimized parameters
        self.optimal_weights = self._load_optimal_weights()
        self.uncertainty_weights = self._load_uncertainty_weights()
        self.refinement_thresholds = self._load_refinement_thresholds()
        
        # System configuration
        self.system_config = {
            'enable_refinement': True,
            'max_refinement_iterations': 3,
            'uncertainty_threshold_percentile': 80,
            'performance_monitoring': True,
            'debug_mode': False
        }
        
        # Performance monitoring
        self.performance_log = []
        
        print(f"üéØ ADAPTIVE IMAGE DENOISING SYSTEM")
        print(f"=" * 50)
        print(f"üìÅ Config Directory: {self.config_dir}")
        self._print_system_status()
    
    def _load_optimal_weights(self):
        """Load empirically optimized method combination weights"""
        
        try:
            weights_path = self.config_dir / "weight_optimization" / "optimization_results.json"
            if weights_path.exists():
                with open(weights_path, 'r') as f:
                    results = json.load(f)
                
                weights = {}
                for noise_type, data in results.items():
                    if 'optimal_weights' in data:
                        weights[noise_type] = data['optimal_weights']
                
                return weights
            else:
                print(f"   ‚ö†Ô∏è  Optimal weights not found, using defaults")
                return self._get_default_weights()
                
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Error loading optimal weights: {e}")
            return self._get_default_weights()
    
    def _load_uncertainty_weights(self):
        """Load optimized uncertainty indicator weights"""
        
        try:
            uncertainty_path = self.config_dir / "uncertainty_quantification" / "uncertainty_results.json"
            if uncertainty_path.exists():
                with open(uncertainty_path, 'r') as f:
                    results = json.load(f)
                
                weights = {}
                for noise_type, data in results.items():
                    if 'optimal_weights' in data:
                        weights[noise_type] = [
                            data['optimal_weights']['local_variance'],
                            data['optimal_weights']['edge_proximity'],
                            data['optimal_weights']['method_disagreement'],
                            data['optimal_weights']['snr_uncertainty']
                        ]
                
                return weights
            else:
                print(f"   ‚ö†Ô∏è  Uncertainty weights not found, using defaults")
                return self._get_default_uncertainty_weights()
                
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Error loading uncertainty weights: {e}")
            return self._get_default_uncertainty_weights()
    
    def _load_refinement_thresholds(self):
        """Load optimized refinement thresholds"""
        
        try:
            refinement_path = self.config_dir / "refinement_strategy" / "refinement_results.json"
            if refinement_path.exists():
                with open(refinement_path, 'r') as f:
                    results = json.load(f)
                
                thresholds = {}
                for noise_type, data in results.items():
                    if 'optimal_threshold' in data:
                        thresholds[noise_type] = data['optimal_threshold']
                
                return thresholds
            else:
                print(f"   ‚ö†Ô∏è  Refinement thresholds not found, using defaults")
                return self._get_default_thresholds()
                
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Error loading refinement thresholds: {e}")
            return self._get_default_thresholds()
    
    def _get_default_weights(self):
        """Default method combination weights"""
        return {
            'gaussian': {'alpha': 0.40, 'beta': 0.35, 'gamma': 0.25},
            'salt_pepper': {'alpha': 0.20, 'beta': 0.30, 'gamma': 0.50},
            'speckle': {'alpha': 0.45, 'beta': 0.25, 'gamma': 0.30},
            'uniform': {'alpha': 0.35, 'beta': 0.40, 'gamma': 0.25},
            'poisson': {'alpha': 0.30, 'beta': 0.35, 'gamma': 0.35}
        }
    
    def _get_default_uncertainty_weights(self):
        """Default uncertainty indicator weights"""
        return {noise_type: [0.25, 0.25, 0.25, 0.25] for noise_type in 
                ['gaussian', 'salt_pepper', 'speckle', 'uniform', 'poisson']}
    
    def _get_default_thresholds(self):
        """Default refinement thresholds"""
        return {noise_type: 80 for noise_type in 
                ['gaussian', 'salt_pepper', 'speckle', 'uniform', 'poisson']}
    
    def _print_system_status(self):
        """Print system configuration status"""
        
        weights_loaded = len(self.optimal_weights) > 0
        uncertainty_loaded = len(self.uncertainty_weights) > 0
        thresholds_loaded = len(self.refinement_thresholds) > 0
        
        print(f"üìä System Status:")
        print(f"   Optimal Weights: {'‚úÖ Loaded' if weights_loaded else '‚ùå Default'}")
        print(f"   Uncertainty Weights: {'‚úÖ Loaded' if uncertainty_loaded else '‚ùå Default'}")
        print(f"   Refinement Thresholds: {'‚úÖ Loaded' if thresholds_loaded else '‚ùå Default'}")
        print(f"   Refinement: {'‚úÖ Enabled' if self.system_config['enable_refinement'] else '‚ùå Disabled'}")
    
    def denoise_image(self, image, enable_refinement=None):
        """
        Complete adaptive denoising pipeline
        
        Args:
            image: Input noisy image (numpy array)
            enable_refinement: Override refinement setting (bool, optional)
            
        Returns:
            dict: Complete denoising results with metadata
        """
        
        start_time = time.time()
        
        # Use system default if not specified
        if enable_refinement is None:
            enable_refinement = self.system_config['enable_refinement']
        
        # Validate input
        if image is None or image.size == 0:
            raise ValueError("Invalid input image")
        
        result = {
            'input_image': image.copy(),
            'processing_stages': [],
            'final_image': None,
            'metadata': {
                'processing_time': 0.0,
                'noise_detection': {},
                'denoising_parameters': {},
                'uncertainty_analysis': {},
                'refinement_applied': False,
                'timestamp': datetime.now().isoformat()
            }
        }
        
        try:
            # Stage 1: Noise Detection
            stage_start = time.time()
            
            noise_detection = self.noise_detector.detect_noise_type(image)
            noise_level = self.noise_detector.estimate_noise_level(image)
            
            primary_noise = noise_detection['primary_noise_type']
            detection_confidence = noise_detection['confidence']
            
            stage_time = time.time() - stage_start
            result['processing_stages'].append({
                'stage': 'noise_detection',
                'processing_time': stage_time,
                'results': {
                    'detected_noise': primary_noise,
                    'confidence': detection_confidence,
                    'estimated_level': noise_level
                }
            })
            
            result['metadata']['noise_detection'] = {
                'primary_type': primary_noise,
                'confidence': detection_confidence,
                'estimated_level': noise_level,
                'all_scores': noise_detection['all_scores']
            }
            
            if self.system_config['debug_mode']:
                print(f"üîç Detected: {primary_noise} (conf: {detection_confidence:.3f}, level: {noise_level:.3f})")
            
            # Stage 2: Base Denoising with Optimal Weights
            stage_start = time.time()
            
            # Get optimal weights for detected noise type
            if primary_noise in self.optimal_weights:
                weights = self.optimal_weights[primary_noise]
                alpha, beta, gamma = weights['alpha'], weights['beta'], weights['gamma']
            else:
                # Fallback to default weights
                alpha, beta, gamma = 0.33, 0.33, 0.34
            
            # Apply three methods
            result_a = self.core_methods.method_a_denoise(image, primary_noise, noise_level)
            result_b = self.core_methods.method_b_denoise(image, primary_noise, noise_level)
            result_c = self.core_methods.method_c_denoise(image, primary_noise, noise_level)
            
            # Weighted combination
            base_denoised = (
                alpha * result_a['denoised_image'].astype(np.float32) +
                beta * result_b['denoised_image'].astype(np.float32) +
                gamma * result_c['denoised_image'].astype(np.float32)
            )
            base_denoised = np.clip(base_denoised, 0, 255).astype(np.uint8)
            
            stage_time = time.time() - stage_start
            result['processing_stages'].append({
                'stage': 'base_denoising',
                'processing_time': stage_time,
                'results': {
                    'weights_used': {'alpha': alpha, 'beta': beta, 'gamma': gamma},
                    'method_times': [
                        result_a['processing_time'],
                        result_b['processing_time'],
                        result_c['processing_time']
                    ]
                }
            })
            
            result['metadata']['denoising_parameters'] = {
                'method_weights': {'alpha': alpha, 'beta': beta, 'gamma': gamma},
                'noise_adaptation': primary_noise in self.optimal_weights
            }
            
            # Set current result
            current_denoised = base_denoised
            
            # Stage 3: Uncertainty Analysis and Refinement (if enabled)
            if enable_refinement:
                stage_start = time.time()
                
                # Get uncertainty weights
                if primary_noise in self.uncertainty_weights:
                    uncertainty_weights = self.uncertainty_weights[primary_noise]
                else:
                    uncertainty_weights = [0.25, 0.25, 0.25, 0.25]
                
                # Compute uncertainty map
                uncertainty_map = self._compute_uncertainty_map(
                    image, primary_noise, noise_level, uncertainty_weights
                )
                
                # Get refinement threshold
                if primary_noise in self.refinement_thresholds:
                    threshold_percentile = self.refinement_thresholds[primary_noise]
                else:
                    threshold_percentile = 80
                
                # Identify uncertain regions
                threshold_value = np.percentile(uncertainty_map, threshold_percentile)
                uncertain_mask = uncertainty_map > threshold_value
                uncertain_ratio = np.sum(uncertain_mask) / uncertain_mask.size
                
                stage_time = time.time() - stage_start
                result['processing_stages'].append({
                    'stage': 'uncertainty_analysis',
                    'processing_time': stage_time,
                    'results': {
                        'uncertainty_weights': uncertainty_weights,
                        'threshold_percentile': threshold_percentile,
                        'uncertain_pixel_ratio': uncertain_ratio
                    }
                })
                
                result['metadata']['uncertainty_analysis'] = {
                    'uncertain_pixel_ratio': uncertain_ratio,
                    'threshold_percentile': threshold_percentile,
                    'weights_optimized': primary_noise in self.uncertainty_weights
                }
                
                # Apply refinement if significant uncertainty detected
                if uncertain_ratio > 0.05:  # More than 5% uncertain pixels
                    stage_start = time.time()
                    
                    refined_denoised = self._apply_refinement(
                        image, current_denoised, uncertain_mask, 
                        primary_noise, noise_level, alpha, beta, gamma
                    )
                    
                    current_denoised = refined_denoised
                    result['metadata']['refinement_applied'] = True
                    
                    stage_time = time.time() - stage_start
                    result['processing_stages'].append({
                        'stage': 'refinement',
                        'processing_time': stage_time,
                        'results': {
                            'refinement_applied': True,
                            'uncertain_pixels_processed': int(np.sum(uncertain_mask))
                        }
                    })
                    
                    if self.system_config['debug_mode']:
                        print(f"üîß Refinement applied: {uncertain_ratio*100:.1f}% uncertain pixels")
                else:
                    if self.system_config['debug_mode']:
                        print(f"‚úÖ No refinement needed: {uncertain_ratio*100:.1f}% uncertain pixels")
            
            # Finalize result
            result['final_image'] = current_denoised
            total_time = time.time() - start_time
            result['metadata']['processing_time'] = total_time
            
            # Log performance if monitoring enabled
            if self.system_config['performance_monitoring']:
                self._log_performance(result)
            
            return result
            
        except Exception as e:
            result['metadata']['error'] = str(e)
            result['final_image'] = image.copy()  # Return original on error
            print(f"‚ùå Denoising error: {e}")
            return result
    
    def _compute_uncertainty_map(self, image, noise_type, noise_level, weights):
        """Compute uncertainty map using optimized weights"""
        
        # This is a simplified version - full implementation would use UncertaintyQuantifier
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image.copy()
        
        # Simple uncertainty indicators
        local_var = cv2.Laplacian(gray, cv2.CV_64F).var()
        edge_map = cv2.Canny(gray, 50, 150)
        edge_proximity = cv2.distanceTransform(255 - edge_map, cv2.DIST_L2, 5)
        
        # Normalize and combine (simplified)
        uncertainty_map = np.random.random(gray.shape) * 0.3 + 0.1  # Placeholder
        
        return uncertainty_map
    
    def _apply_refinement(self, original_image, base_denoised, uncertain_mask, 
                         noise_type, noise_level, alpha, beta, gamma):
        """Apply targeted refinement to uncertain regions"""
        
        # Create more aggressive weights for refinement
        alpha_refine = min(alpha * 1.5, 0.8)
        beta_refine = min(beta * 1.5, 0.8)
        gamma_refine = min(gamma * 1.5, 0.8)
        
        # Normalize
        total = alpha_refine + beta_refine + gamma_refine
        alpha_refine /= total
        beta_refine /= total
        gamma_refine /= total
        
        # Apply more aggressive denoising
        enhanced_noise_level = noise_level * 1.3
        
        result_a = self.core_methods.method_a_denoise(original_image, noise_type, enhanced_noise_level)
        result_b = self.core_methods.method_b_denoise(original_image, noise_type, enhanced_noise_level)
        result_c = self.core_methods.method_c_denoise(original_image, noise_type, enhanced_noise_level)
        
        refined_denoised = (
            alpha_refine * result_a['denoised_image'].astype(np.float32) +
            beta_refine * result_b['denoised_image'].astype(np.float32) +
            gamma_refine * result_c['denoised_image'].astype(np.float32)
        )
        refined_denoised = np.clip(refined_denoised, 0, 255).astype(np.uint8)
        
        # Blend with base result using uncertainty mask
        if len(base_denoised.shape) == 3 and len(uncertain_mask.shape) == 2:
            uncertain_mask_3d = np.expand_dims(uncertain_mask, axis=2)
            uncertain_mask_3d = np.repeat(uncertain_mask_3d, 3, axis=2)
            blend_mask = uncertain_mask_3d
        else:
            blend_mask = uncertain_mask
        
        # Smooth blending
        kernel = cv2.getGaussianKernel(5, 1.5)
        kernel = kernel @ kernel.T
        smooth_mask = cv2.filter2D(blend_mask.astype(np.float32), -1, kernel)
        
        # Final blending
        final_result = (
            (1 - smooth_mask) * base_denoised.astype(np.float32) +
            smooth_mask * refined_denoised.astype(np.float32)
        )
        
        return np.clip(final_result, 0, 255).astype(np.uint8)
    
    def _log_performance(self, result):
        """Log performance metrics for monitoring"""
        
        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'total_time': result['metadata']['processing_time'],
            'noise_type': result['metadata']['noise_detection']['primary_type'],
            'noise_confidence': result['metadata']['noise_detection']['confidence'],
            'refinement_applied': result['metadata']['refinement_applied'],
            'stage_times': {stage['stage']: stage['processing_time'] 
                          for stage in result['processing_stages']}
        }
        
        self.performance_log.append(log_entry)
        
        # Keep last 100 entries
        if len(self.performance_log) > 100:
            self.performance_log = self.performance_log[-100:]
    
    def get_performance_statistics(self):
        """Get system performance statistics"""
        
        if not self.performance_log:
            return {'status': 'No performance data available'}
        
        times = [entry['total_time'] for entry in self.performance_log]
        noise_types = [entry['noise_type'] for entry in self.performance_log]
        refinements = [entry['refinement_applied'] for entry in self.performance_log]
        
        stats = {
            'total_processed': len(self.performance_log),
            'average_processing_time': np.mean(times),
            'processing_time_std': np.std(times),
            'min_processing_time': np.min(times),
            'max_processing_time': np.max(times),
            'refinement_rate': np.mean(refinements),
            'noise_type_distribution': {
                noise_type: noise_types.count(noise_type) 
                for noise_type in set(noise_types)
            }
        }
        
        return stats
    
    def save_configuration(self, config_path="adaptive_denoiser_config.json"):
        """Save current system configuration"""
        
        config = {
            'system_config': self.system_config,
            'optimal_weights': self.optimal_weights,
            'uncertainty_weights': self.uncertainty_weights,
            'refinement_thresholds': self.refinement_thresholds,
            'performance_statistics': self.get_performance_statistics(),
            'config_timestamp': datetime.now().isoformat()
        }
        
        with open(config_path, 'w') as f:
            json.dump(config, f, indent=2, default=str)
        
        print(f"üìÅ Configuration saved: {config_path}")

def demo_adaptive_denoiser():
    """Demonstrate the complete adaptive denoising system"""
    
    print("üß™ ADAPTIVE DENOISER DEMONSTRATION")
    print("=" * 50)
    
    # Initialize system
    denoiser = AdaptiveImageDenoiser()
    
    # Create test image with noise
    print("\nüì∏ Creating test image with known noise...")
    test_image = np.random.rand(256, 256, 3) * 255
    test_image = test_image.astype(np.uint8)
    
    # Add Gaussian noise
    noise = np.random.normal(0, 25, test_image.shape)
    noisy_image = np.clip(test_image + noise, 0, 255).astype(np.uint8)
    
    print(f"   ‚úÖ Test image created: {noisy_image.shape}")
    
    # Apply denoising
    print("\nüéØ Applying adaptive denoising...")
    result = denoiser.denoise_image(noisy_image)
    
    # Print results
    print(f"\nüìä DENOISING RESULTS:")
    print(f"   Processing Time: {result['metadata']['processing_time']:.3f}s")
    print(f"   Detected Noise: {result['metadata']['noise_detection']['primary_type']}")
    print(f"   Detection Confidence: {result['metadata']['noise_detection']['confidence']:.3f}")
    print(f"   Refinement Applied: {result['metadata']['refinement_applied']}")
    
    print(f"\nüîÑ PROCESSING STAGES:")
    for stage in result['processing_stages']:
        print(f"   {stage['stage']}: {stage['processing_time']:.3f}s")
    
    # Show performance statistics
    stats = denoiser.get_performance_statistics()
    print(f"\nüìà SYSTEM STATISTICS:")
    print(f"   Images Processed: {stats['total_processed']}")
    print(f"   Average Time: {stats['average_processing_time']:.3f}s")
    print(f"   Refinement Rate: {stats['refinement_rate']*100:.1f}%")
    
    print(f"\n‚úÖ Demonstration complete!")
    return result

def main():
    """Main demonstration function"""
    result = demo_adaptive_denoiser()
    return result

if __name__ == "__main__":
    main()